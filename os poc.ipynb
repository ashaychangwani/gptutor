{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.error import RateLimitError\n",
    "import openai\n",
    "import backoff\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from transformers import GPT2TokenizerFast\n",
    "from typing import List\n",
    "from ratelimit import limits,sleep_and_retry\n",
    "from time import sleep\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "COMPLETIONS_MODEL = \"text-davinci-003\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPLETIONS_MODEL = \"text-davinci-003\"\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "openai.api_key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@backoff.on_exception(backoff.expo, RateLimitError)\n",
    "def get_embedding(text: str, model: str=EMBEDDING_MODEL, idx: int=0) -> list[float]:\n",
    "    result = openai.Embedding.create(\n",
    "    model=model,\n",
    "    input=text\n",
    "    )\n",
    "\n",
    "    return result[\"data\"][0][\"embedding\"]\n",
    "\n",
    "def compute_doc_embeddings(df: pd.DataFrame) -> dict[tuple[str, str], list[float]]:\n",
    "    \"\"\"\n",
    "    Create an embedding for each row in the dataframe using the OpenAI Embeddings API.\n",
    "    \n",
    "    Return a dictionary that maps between each embedding vector and the index of the row that it corresponds to.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        idx: get_embedding(r.content) for idx, r in df.iterrows()\n",
    "    }\n",
    "    \n",
    "def compute_text_embeddings(text: str, start_index:int = 0) -> dict[tuple[str, str], list[float]]:\n",
    "    return {\n",
    "        (start_index+idx): get_embedding(line, EMBEDDING_MODEL ,idx) for idx, line in enumerate(text)\n",
    "    }\n",
    "\n",
    "def update_text_embeddings(compute_embedding, text, new_text):\n",
    "    \n",
    "    compute_embedding.update(compute_text_embeddings(new_text, len(compute_embedding)))\n",
    "    return text + new_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(fname: str):\n",
    "    \"\"\"\n",
    "    Read the document embeddings and their keys from a CSV.\n",
    "    \n",
    "    fname is the path to a CSV with exactly these named columns: \n",
    "        \"title\", \"heading\", \"0\", \"1\", ... up to the length of the embedding vectors.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(fname, header=0)\n",
    "    max_dim = max([int(c) for c in df.columns if c != \"title\" and c != \"heading\"])\n",
    "    return {\n",
    "           (r.title, r.heading): [r[str(i)] for i in range(max_dim + 1)] for _, r in df.iterrows()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n"
     ]
    }
   ],
   "source": [
    "text = open (\"nyush.txt\", \"r\").read().split(\"\\n\\n\")\n",
    "\n",
    "# document_embeddings = load_embeddings(\"olympics_sections_document_embeddings.csv\")\n",
    "\n",
    "# context_embeddings = compute_doc_embeddings(df)\n",
    "\n",
    "# context_embeddings = compute_text_embeddings(text)\n",
    "# with open('nyush_embeddings.obj', 'wb') as fp:\n",
    "# \tpickle.dump(context_embeddings, fp)\n",
    "context_embeddings = pickle.load(open('nyush_embeddings.obj', \"rb\"))\n",
    "# print(len(context_embeddings))\n",
    "\n",
    "update = open (\"update.txt\", \"r\").read().split(\"\\n\\n\")\n",
    "\n",
    "text = update_text_embeddings(context_embeddings, text, update)\n",
    "print(len(context_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_similarity(x: List[float], y: List[float]) -> float:\n",
    "    \"\"\"\n",
    "    We could use cosine similarity or dot product to calculate the similarity between vectors.\n",
    "    In practice, we have found it makes little difference. \n",
    "    \"\"\"\n",
    "    return np.dot(np.array(x), np.array(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_similarity(x: list[float], y: list[float]) -> float:\n",
    "    \"\"\"\n",
    "    Returns the similarity between two vectors.\n",
    "    \n",
    "    Because OpenAI Embeddings are normalized to length 1, the cosine similarity is the same as the dot product.\n",
    "    \"\"\"\n",
    "    return np.dot(np.array(x), np.array(y))\n",
    "\n",
    "def order_document_sections_by_query_similarity(query: str, contexts: dict[(str, str), np.array]) -> list[(float, (str, str))]:\n",
    "    \"\"\"\n",
    "    Find the query embedding for the supplied query, and compare it against all of the pre-calculated document embeddings\n",
    "    to find the most relevant sections. \n",
    "    \n",
    "    Return the list of document sections, sorted by relevance in descending order.\n",
    "    \"\"\"\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    document_similarities = sorted([\n",
    "        (vector_similarity(query_embedding, doc_embedding), doc_index) for doc_index, doc_embedding in contexts.items()\n",
    "    ], reverse=True)\n",
    "    \n",
    "    return document_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Context separator contains 3 tokens'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SECTION_LEN = 500\n",
    "SEPARATOR = \"\\n* \"\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "separator_len = len(tokenizer.tokenize(SEPARATOR))\n",
    "\n",
    "f\"Context separator contains {separator_len} tokens\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_prompt(question: str, context_embeddings: dict, df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Fetch relevant \n",
    "    \"\"\"\n",
    "    most_relevant_document_sections = order_document_sections_by_query_similarity(question, context_embeddings)\n",
    "    \n",
    "    chosen_sections = []\n",
    "    chosen_sections_len = 0\n",
    "    chosen_sections_indexes = []\n",
    "     \n",
    "    for _, section_index in most_relevant_document_sections:\n",
    "        # Add contexts until we run out of space.        \n",
    "        document_section = df.loc[section_index]\n",
    "        \n",
    "        chosen_sections_len += document_section.tokens + separator_len\n",
    "        if chosen_sections_len > MAX_SECTION_LEN:\n",
    "            break\n",
    "            \n",
    "        chosen_sections.append(SEPARATOR + document_section.content.replace(\"\\n\", \" \"))\n",
    "        chosen_sections_indexes.append(str(section_index))\n",
    "            \n",
    "    # Useful diagnostic information\n",
    "    print(f\"Selected {len(chosen_sections)} document sections:\")\n",
    "    print(\"\\n\".join(chosen_sections_indexes))\n",
    "    \n",
    "    header = \"\"\"Answer the question as truthfully as possible using the provided context, and if the answer is not contained within the text below, say \"I don't know.\"\\n\\nContext:\\n\"\"\"\n",
    "    \n",
    "    return header + \"\".join(chosen_sections) + \"\\n\\n Q: \" + question + \"\\n A:\"\n",
    "\n",
    "def construct_prompt_with_text(question: str, context_embeddings: dict, text: list, previous_context: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Fetch relevant \n",
    "    \"\"\"\n",
    "    most_relevant_document_sections = order_document_sections_by_query_similarity(question, context_embeddings)\n",
    "    \n",
    "    chosen_sections = []\n",
    "    chosen_sections_len = 0\n",
    "    chosen_sections_indexes = []\n",
    "     \n",
    "    for _, section_index in most_relevant_document_sections:\n",
    "        # Add contexts until we run out of space.        \n",
    "        document_section = text[section_index]\n",
    "        \n",
    "        chosen_sections_len += len(document_section.split()) + separator_len\n",
    "        if chosen_sections_len > MAX_SECTION_LEN:\n",
    "            break\n",
    "            \n",
    "        chosen_sections.append(SEPARATOR + document_section)\n",
    "        chosen_sections_indexes.append(str(section_index))\n",
    "            \n",
    "    # Useful diagnostic information\n",
    "    # print(f\"Selected {len(chosen_sections)} document sections:\")\n",
    "    # print(\"\\n\".join(chosen_sections_indexes))\n",
    "    \n",
    "    header = \"\"\"Answer the question as truthfully as possible using the provided context, and if the answer is not contained within the text below, say \"I don't know.\"\\n\\nContext:\\n\"\"\"\n",
    "    \n",
    "    prompt = header + \"\".join(chosen_sections) \n",
    "    if previous_context is not None:\n",
    "        prompt = prompt + \"\\n\\nPrevious context:\\n\" + previous_context\n",
    "        \n",
    "    prompt = prompt +\"\\n\\n Q: \" + question + \"\\n A:\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===\n",
      " Answer the question as truthfully as possible using the provided context, and if the answer is not contained within the text below, say \"I don't know.\"\n",
      "\n",
      "Context:\n",
      "\n",
      "* Submission\n",
      "You must submit a .zip archive containing all files needed to compile nyush in the root of the archive. You can create the archive file with the following command in the Docker container:\n",
      "* You need to upload the .zip archive to Gradescope. If you need to acknowledge any influences per our academic integrity policy, write them as comments in your source code.\n",
      "* How to get started?\n",
      "Please review our academic integrity policy carefully before you start.\n",
      "* Hello World;\n",
      "* Here are some additional hints:\n",
      "* Milestone 1. Write a simple program that prints the prompt and flushes STDOUT. You may use the getcwd() system call to get the current working directory.\n",
      "* Compilation\n",
      "We will grade your submission in an x86_64 Rocky Linux 8 container on Gradescope. We will compile your program using gcc 12.1.1 with the C17 standard and GNU extensions.\n",
      "* You must provide a Makefile, and by running make, it should generate an executable file named nyush in the current working directory. (Refer to Lab 1 for an example of the Makefile.)\n",
      "* Milestone 2. Write a loop that repeatedly prints the prompt and gets the user input. You may use the getline() library function to obtain user input.\n",
      "* Here is how I would tackle this lab:\n",
      "* The prompt\n",
      "The prompt is what the shell prints before waiting for you to enter a command. In this lab, your prompt must have exactly the following format:\n",
      "* Please make sure that your shell prompt and all error messages are exactly as specified in this document. Any discrepancy may lead to point deductions.\n",
      "* Lab 2: Shell\n",
      "Table of Contents\n",
      "Introduction\n",
      "Objectives\n",
      "Overview\n",
      "Specifications\n",
      "The prompt\n",
      "The command\n",
      "Locating programs\n",
      "Process termination and suspension\n",
      "Signal handling\n",
      "I/O redirection\n",
      "Built-in commands\n",
      "cd <dir>\n",
      "jobs\n",
      "fg <index>\n",
      "exit\n",
      "Compilation\n",
      "Evaluation\n",
      "Submission\n",
      "Rubric\n",
      "Tips\n",
      "Introduction\n",
      "The shell is the main command-line interface between a user and the operating system, and it is an essential part of the daily lives of computer scientists, software engineers, system administrators, and such. It makes heavy use of many OS features. In this lab, you will build a simplified version of the Unix shell called the New Yet Usable SHell, or nyush for short.\n",
      "* Milestone 6. Handle output redirection, such as cat > output.txt.\n",
      "* Milestone 7. Handle input redirection, such as cat < input.txt.\n",
      "* This lab requires you to write a complete system from scratch, so it may be daunting at first. Remember to get the basic functionality working first, and build up your shell step-by-step.\n",
      "* $ zip nyush.zip Makefile *.h *.c\n",
      "Note that other file formats (e.g., rar) will not be accepted.\n",
      "\n",
      " Q: Where do I submit?\n",
      " A:\n"
     ]
    }
   ],
   "source": [
    "prompt = construct_prompt_with_text(\n",
    "    \"Where do I submit?\",\n",
    "    context_embeddings,\n",
    "    text\n",
    ")\n",
    "\n",
    "print(\"===\\n\", prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPLETIONS_API_PARAMS = {\n",
    "    # We use temperature of 0.0 because it gives the most predictable, factual answer.\n",
    "    \"temperature\": 0.0,\n",
    "    \"max_tokens\": 300,\n",
    "    \"model\": COMPLETIONS_MODEL,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query_with_context(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    document_embeddings,\n",
    "    previous_context = None,\n",
    "    show_prompt: bool = True\n",
    ") -> str:\n",
    "    prompt = construct_prompt_with_text(\n",
    "        query,\n",
    "        document_embeddings,\n",
    "        df,\n",
    "        previous_context\n",
    "    )\n",
    "    \n",
    "    if show_prompt:\n",
    "        with open('temp.txt', 'w') as f:\n",
    "            for line in text:\n",
    "                f.write(f\"{line}\\n\")\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "                prompt=prompt,\n",
    "                **COMPLETIONS_API_PARAMS\n",
    "            )\n",
    "\n",
    "    return response[\"choices\"][0][\"text\"].strip(\" \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He was 21 years old.\n"
     ]
    }
   ],
   "source": [
    "previous_context = \"\"\"Q: Where do I submit?\n",
    "A: You need to upload the .zip archive to Gradescope.\n",
    "\n",
    "Q: What is contained in it?\n",
    "A: The .zip archive should contain your source code, Makefile, and any other files necessary for compilation and execution.\n",
    "\n",
    "Q: Show me how the fg command should work?\n",
    "A: The fg command resumes a job in the foreground. It takes exactly one argument: the job index, which is the number inside the bracket printed by the jobs command. For example: [nyush lab2]$ fg 2.\n",
    "\n",
    "Q: Who won the last F1 championship? \n",
    "A: Max Verstappen won the 2022 F1 championship.\n",
    "\n",
    "All terminal outputs should be surrounded by ``` before and after.\n",
    "\"\"\"\n",
    "\n",
    "answer= answer_query_with_context(\"How old was he when he won?\",text, context_embeddings, previous_context)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e49fb333187636713f8f1e15b9093b9b95363117e0a453d86722732007929fff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
