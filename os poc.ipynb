{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.error import RateLimitError\n",
    "import openai\n",
    "import backoff\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from transformers import GPT2TokenizerFast\n",
    "from typing import List\n",
    "from ratelimit import limits,sleep_and_retry\n",
    "from time import sleep\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "COMPLETIONS_MODEL = \"text-davinci-003\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPLETIONS_MODEL = \"text-davinci-003\"\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "openai.api_key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "@backoff.on_exception(backoff.expo, RateLimitError)\n",
    "def get_embedding(text: str, model: str=EMBEDDING_MODEL, idx: int=0) -> list[float]:\n",
    "    result = openai.Embedding.create(\n",
    "    model=model,\n",
    "    input=text\n",
    "    )\n",
    "\n",
    "    return result[\"data\"][0][\"embedding\"]\n",
    "\n",
    "def compute_doc_embeddings(df: pd.DataFrame) -> dict[tuple[str, str], list[float]]:\n",
    "    \"\"\"\n",
    "    Create an embedding for each row in the dataframe using the OpenAI Embeddings API.\n",
    "    \n",
    "    Return a dictionary that maps between each embedding vector and the index of the row that it corresponds to.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        idx: get_embedding(r.content) for idx, r in df.iterrows()\n",
    "    }\n",
    "    \n",
    "def compute_text_embeddings(text: str) -> dict[tuple[str, str], list[float]]:\n",
    "    return {\n",
    "        idx: get_embedding(line, EMBEDDING_MODEL ,idx) for idx, line in enumerate(text)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(fname: str):\n",
    "    \"\"\"\n",
    "    Read the document embeddings and their keys from a CSV.\n",
    "    \n",
    "    fname is the path to a CSV with exactly these named columns: \n",
    "        \"title\", \"heading\", \"0\", \"1\", ... up to the length of the embedding vectors.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(fname, header=0)\n",
    "    max_dim = max([int(c) for c in df.columns if c != \"title\" and c != \"heading\"])\n",
    "    return {\n",
    "           (r.title, r.heading): [r[str(i)] for i in range(max_dim + 1)] for _, r in df.iterrows()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document_embeddings = load_embeddings(\"olympics_sections_document_embeddings.csv\")\n",
    "\n",
    "# context_embeddings = compute_doc_embeddings(df)\n",
    "\n",
    "# context_embeddings = compute_text_embeddings(open (\"nyush.txt\", \"r\").read().splitlines())\n",
    "context_embeddings = pickle.load(open('nyush_embeddings.obj', \"rb\"))\n",
    "\n",
    "# print(context_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_similarity(x: List[float], y: List[float]) -> float:\n",
    "    \"\"\"\n",
    "    We could use cosine similarity or dot product to calculate the similarity between vectors.\n",
    "    In practice, we have found it makes little difference. \n",
    "    \"\"\"\n",
    "    return np.dot(np.array(x), np.array(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_similarity(x: list[float], y: list[float]) -> float:\n",
    "    \"\"\"\n",
    "    Returns the similarity between two vectors.\n",
    "    \n",
    "    Because OpenAI Embeddings are normalized to length 1, the cosine similarity is the same as the dot product.\n",
    "    \"\"\"\n",
    "    return np.dot(np.array(x), np.array(y))\n",
    "\n",
    "def order_document_sections_by_query_similarity(query: str, contexts: dict[(str, str), np.array]) -> list[(float, (str, str))]:\n",
    "    \"\"\"\n",
    "    Find the query embedding for the supplied query, and compare it against all of the pre-calculated document embeddings\n",
    "    to find the most relevant sections. \n",
    "    \n",
    "    Return the list of document sections, sorted by relevance in descending order.\n",
    "    \"\"\"\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    document_similarities = sorted([\n",
    "        (vector_similarity(query_embedding, doc_embedding), doc_index) for doc_index, doc_embedding in contexts.items()\n",
    "    ], reverse=True)\n",
    "    \n",
    "    return document_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.7486145040198118, 206),\n",
       " (0.7477867066522546, 191),\n",
       " (0.746245176559252, 177),\n",
       " (0.744552465070722, 126),\n",
       " (0.7422233607528219, 144)]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_document_sections_by_query_similarity(\"Who won the men's high jump?\", context_embeddings)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Context separator contains 3 tokens'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SECTION_LEN = 500\n",
    "SEPARATOR = \"\\n* \"\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "separator_len = len(tokenizer.tokenize(SEPARATOR))\n",
    "\n",
    "f\"Context separator contains {separator_len} tokens\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_prompt(question: str, context_embeddings: dict, df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Fetch relevant \n",
    "    \"\"\"\n",
    "    most_relevant_document_sections = order_document_sections_by_query_similarity(question, context_embeddings)\n",
    "    \n",
    "    chosen_sections = []\n",
    "    chosen_sections_len = 0\n",
    "    chosen_sections_indexes = []\n",
    "     \n",
    "    for _, section_index in most_relevant_document_sections:\n",
    "        # Add contexts until we run out of space.        \n",
    "        document_section = df.loc[section_index]\n",
    "        \n",
    "        chosen_sections_len += document_section.tokens + separator_len\n",
    "        if chosen_sections_len > MAX_SECTION_LEN:\n",
    "            break\n",
    "            \n",
    "        chosen_sections.append(SEPARATOR + document_section.content.replace(\"\\n\", \" \"))\n",
    "        chosen_sections_indexes.append(str(section_index))\n",
    "            \n",
    "    # Useful diagnostic information\n",
    "    print(f\"Selected {len(chosen_sections)} document sections:\")\n",
    "    print(\"\\n\".join(chosen_sections_indexes))\n",
    "    \n",
    "    header = \"\"\"Answer the question as truthfully as possible using the provided context, and if the answer is not contained within the text below, say \"I don't know.\"\\n\\nContext:\\n\"\"\"\n",
    "    \n",
    "    return header + \"\".join(chosen_sections) + \"\\n\\n Q: \" + question + \"\\n A:\"\n",
    "\n",
    "def construct_prompt_with_text(question: str, context_embeddings: dict, text: list) -> str:\n",
    "    \"\"\"\n",
    "    Fetch relevant \n",
    "    \"\"\"\n",
    "    most_relevant_document_sections = order_document_sections_by_query_similarity(question, context_embeddings)\n",
    "    \n",
    "    chosen_sections = []\n",
    "    chosen_sections_len = 0\n",
    "    chosen_sections_indexes = []\n",
    "     \n",
    "    for _, section_index in most_relevant_document_sections:\n",
    "        # Add contexts until we run out of space.        \n",
    "        document_section = text[section_index]\n",
    "        \n",
    "        chosen_sections_len += len(document_section.split()) + separator_len\n",
    "        if chosen_sections_len > MAX_SECTION_LEN:\n",
    "            break\n",
    "            \n",
    "        chosen_sections.append(SEPARATOR + document_section.replace(\"\\n\", \" \"))\n",
    "        chosen_sections_indexes.append(str(section_index))\n",
    "            \n",
    "    # Useful diagnostic information\n",
    "    print(f\"Selected {len(chosen_sections)} document sections:\")\n",
    "    print(\"\\n\".join(chosen_sections_indexes))\n",
    "    \n",
    "    header = \"\"\"Answer the question as truthfully as possible using the provided context, and if the answer is not contained within the text below, say \"I don't know.\"\\n\\nContext:\\n\"\"\"\n",
    "    \n",
    "    return header + \"\".join(chosen_sections) + \"\\n\\n Q: \" + question + \"\\n A:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 40 document sections:\n",
      "209\n",
      "191\n",
      "15\n",
      "116\n",
      "93\n",
      "195\n",
      "206\n",
      "30\n",
      "24\n",
      "247\n",
      "192\n",
      "184\n",
      "201\n",
      "196\n",
      "6\n",
      "26\n",
      "144\n",
      "60\n",
      "187\n",
      "59\n",
      "0\n",
      "238\n",
      "198\n",
      "213\n",
      "13\n",
      "3\n",
      "103\n",
      "126\n",
      "185\n",
      "73\n",
      "194\n",
      "17\n",
      "249\n",
      "114\n",
      "234\n",
      "75\n",
      "214\n",
      "117\n",
      "115\n",
      "11\n",
      "===\n",
      " Answer the question as truthfully as possible using the provided context, and if the answer is not contained within the text below, say \"I don't know.\"\n",
      "\n",
      "Context:\n",
      "\n",
      "* How to get started?\n",
      "* Submission\n",
      "* The prompt\n",
      "* Input redirection\n",
      "* Locating programs\n",
      "* You need to upload the .zip archive to Gradescope. If you need to acknowledge any influences per our academic integrity policy, write them as comments in your source code.\n",
      "* Tips\n",
      "* The command\n",
      "* For example, if you are in /home/abc123/cs202/lab2, then the prompt should be:\n",
      "* Here are some additional hints:\n",
      "* You must submit a .zip archive containing all files needed to compile nyush in the root of the archive. You can create the archive file with the following command in the Docker container:\n",
      "* We will grade your submission in an x86_64 Rocky Linux 8 container on Gradescope. We will compile your program using gcc 12.1.1 with the C17 standard and GNU extensions.\n",
      "* Input and output redirection. (10 points)\n",
      "* Rubric\n",
      "* Learn how to write an interactive command-line program.\n",
      "* If you are in the root directory (/), then the prompt should be:\n",
      "* jobs\n",
      "* A blank line.\n",
      "* Evaluation\n",
      "* Here are some examples of valid commands:\n",
      "* Introduction\n",
      "* Parsing the command line\n",
      "* Compile successfully and can print the correct prompt. (40 points)\n",
      "* Milestone 1. Write a simple program that prints the prompt and flushes STDOUT. You may use the getcwd() system call to get the current working directory.\n",
      "* Specifications\n",
      "* Objectives\n",
      "* Signal handling\n",
      "* Pipe\n",
      "* You must provide a Makefile, and by running make, it should generate an executable file named nyush in the current working directory. (Refer to Lab 1 for an example of the Makefile.)\n",
      "* cat <\n",
      "* Note that other file formats (e.g., rar) will not be accepted.\n",
      "* An opening bracket [.\n",
      "* I’m still confused about suspended jobs. Any more hints?\n",
      "* I/O redirection\n",
      "* Reading man pages\n",
      "* cat |\n",
      "* Milestone 2. Write a loop that repeatedly prints the prompt and gets the user input. You may use the getline() library function to obtain user input.\n",
      "* Input redirection is achieved by a < symbol followed by a filename. For example:\n",
      "* Sometimes, a user would read the input to a program from a file rather than the keyboard, or send the output of a program to a file rather than the screen. Your shell should be able to redirect the standard input (STDIN) and the standard output (STDOUT). For simplicity, you are not required to redirect the standard error (STDERR).\n",
      "* Overview\n",
      "\n",
      " Q: Where do I submit?\n",
      " A:\n"
     ]
    }
   ],
   "source": [
    "prompt = construct_prompt_with_text(\n",
    "    \"Where do I submit?\",\n",
    "    context_embeddings,\n",
    "    open (\"nyush.txt\", \"r\").read().splitlines()\n",
    ")\n",
    "\n",
    "print(\"===\\n\", prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPLETIONS_API_PARAMS = {\n",
    "    # We use temperature of 0.0 because it gives the most predictable, factual answer.\n",
    "    \"temperature\": 0.0,\n",
    "    \"max_tokens\": 300,\n",
    "    \"model\": COMPLETIONS_MODEL,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query_with_context(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    document_embeddings,\n",
    "    show_prompt: bool = False\n",
    ") -> str:\n",
    "    prompt = construct_prompt_with_text(\n",
    "        query,\n",
    "        document_embeddings,\n",
    "        df\n",
    "    )\n",
    "    print(prompt)\n",
    "    \n",
    "    if show_prompt:\n",
    "        print(prompt)\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "                prompt=prompt,\n",
    "                **COMPLETIONS_API_PARAMS\n",
    "            )\n",
    "\n",
    "    return response[\"choices\"][0][\"text\"].strip(\" \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 71 document sections:\n",
      "191\n",
      "216\n",
      "250\n",
      "248\n",
      "75\n",
      "11\n",
      "187\n",
      "13\n",
      "206\n",
      "177\n",
      "0\n",
      "126\n",
      "19\n",
      "30\n",
      "60\n",
      "144\n",
      "15\n",
      "23\n",
      "22\n",
      "21\n",
      "196\n",
      "73\n",
      "3\n",
      "209\n",
      "183\n",
      "251\n",
      "17\n",
      "92\n",
      "74\n",
      "18\n",
      "247\n",
      "76\n",
      "139\n",
      "107\n",
      "153\n",
      "249\n",
      "35\n",
      "169\n",
      "163\n",
      "147\n",
      "103\n",
      "116\n",
      "93\n",
      "27\n",
      "238\n",
      "91\n",
      "172\n",
      "150\n",
      "108\n",
      "25\n",
      "207\n",
      "166\n",
      "198\n",
      "174\n",
      "141\n",
      "90\n",
      "87\n",
      "181\n",
      "159\n",
      "202\n",
      "245\n",
      "99\n",
      "234\n",
      "197\n",
      "212\n",
      "100\n",
      "190\n",
      "106\n",
      "59\n",
      "2\n",
      "176\n",
      "Answer the question as truthfully as possible using the provided context, and if the answer is not contained within the text below, say \"I don't know.\"\n",
      "\n",
      "Context:\n",
      "\n",
      "* Submission\n",
      "* Click to reveal spoiler\n",
      "* Click to reveal spoiler\n",
      "* Click to reveal spoiler\n",
      "* cat |\n",
      "* Overview\n",
      "* Evaluation\n",
      "* Specifications\n",
      "* Tips\n",
      "* exit\n",
      "* Introduction\n",
      "* Pipe\n",
      "* A whitespace.\n",
      "* The command\n",
      "* A blank line.\n",
      "* jobs\n",
      "* The prompt\n",
      "* Another whitespace.\n",
      "* A dollar sign $.\n",
      "* A closing bracket ].\n",
      "* Rubric\n",
      "* cat <\n",
      "* Objectives\n",
      "* How to get started?\n",
      "* Compilation\n",
      "* This lab has borrowed some ideas from Prof. Arpaci-Dusseau and Dr. T. Y. Wong.\n",
      "* An opening bracket [.\n",
      "* (Again, the final █ character represents your cursor.)\n",
      "* cat >\n",
      "* The word nyush.\n",
      "* Here are some additional hints:\n",
      "* | cat\n",
      "* [nyush bin]$ █\n",
      "* ^C\n",
      "* For simplicity, we have the following assumptions:\n",
      "* I’m still confused about suspended jobs. Any more hints?\n",
      "* For simplicity, our test cases have the following assumptions:\n",
      "* [1] ./hello\n",
      "* [1] ./hello\n",
      "* [1] ./hello\n",
      "* Signal handling\n",
      "* Input redirection\n",
      "* Locating programs\n",
      "* [nyush /]$ █\n",
      "* Parsing the command line\n",
      "* [nyush lab2]$ █\n",
      "* [nyush lab2]$ █\n",
      "* [nyush lab2]$ █\n",
      "* [nyush lab2]$ █\n",
      "* [nyush lab2]$ █\n",
      "* Don’t procrastinate! Don’t procrastinate!! Don’t procrastinate!!!\n",
      "* [nyush lab2]$ fg 2\n",
      "* Compile successfully and can print the correct prompt. (40 points)\n",
      "* Error: invalid command\n",
      "* Error: invalid command\n",
      "* Error: invalid command\n",
      "* Error: invalid command\n",
      "* Error: invalid command\n",
      "* Error: invalid command\n",
      "* Pipes. (10 points)\n",
      "* I’m still confused about pipes. Any more hints?\n",
      "* Error: invalid program\n",
      "* Reading man pages\n",
      "* The total of this lab is 100 points, mapped to 15% of your final grade of this course.\n",
      "* Here is how I would tackle this lab:\n",
      "* Process termination and suspension\n",
      "* Note that these test cases are not exhaustive. The test cases on Gradescope are different from the ones provided and will not be disclosed. Do not try to hack or exploit the autograder.\n",
      "* [nyush lab2]$ cat\n",
      "* Here are some examples of valid commands:\n",
      "* \n",
      "* Error: invalid job\n",
      "\n",
      " Q: Who won the F1 championship last year?\n",
      " A:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I don't know.\""
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_query_with_context(\"Who won the F1 championship last year?\",open (\"nyush.txt\", \"r\").read().splitlines(), context_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e49fb333187636713f8f1e15b9093b9b95363117e0a453d86722732007929fff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
